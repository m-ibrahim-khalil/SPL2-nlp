{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SPL2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bsse1009/SPL2-nlp/blob/master/full_model/SPL2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIZQNYP40Lrj",
        "outputId": "d5de4ba1-2277-41f1-85c1-8daa2858b6df"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJQWsu8d0rXv"
      },
      "source": [
        "import tensorflow as tf\n",
        "with tf.device('/device:GPU:0'):\n",
        "  !cp \"drive/My Drive/app/models/bidaf.py\" .\n",
        "  !cp \"drive/My Drive/app/data/magnitude/wiki-news-300d-1M.magnitude\" .\n",
        "  !cp \"drive/My Drive/app/data/magnitude/glove_medium_glove.6B.50d.magnitude.tmp\" .\n",
        "  # !cp \"drive/My Drive/app/data/preproccess_squad/dev.answer\" .\n",
        "  # !cp \"drive/My Drive/app/data/preproccess_squad/dev.context\" .\n",
        "  # !cp \"drive/My Drive/app/data/preproccess_squad/dev.question\" .\n",
        "  # !cp \"drive/My Drive/app/data/preproccess_squad/dev.span\" .\n",
        "  # !cp \"drive/My Drive/app/data/preproccess_squad/train_3.answer\" .\n",
        "  !cp \"drive/My Drive/app/data/preproccess_squad/train_6.context\" .\n",
        "  !cp \"drive/My Drive/app/data/preproccess_squad/train_6.question\" .\n",
        "  !cp \"drive/My Drive/app/data/preproccess_squad/train_6.span\" .\n",
        "  !cp \"drive/My Drive/app/data/preproccess_squad/squad_preproccess.py\" .\n",
        "  !cp \"drive/My Drive/app/data/preproccess_squad/download_glove.py\" .\n",
        "  !cp \"drive/My Drive/app/layers/BiDAF.py\" .\n",
        "  !cp \"drive/My Drive/app/layers/Modelling.py\" .\n",
        "  !cp \"drive/My Drive/app/layers/contexual_embedding.py\" .\n",
        "  !cp \"drive/My Drive/app/layers/highway_layer.py\" .\n",
        "  !cp \"drive/My Drive/app/layers/output.py\" .\n",
        "  !cp \"drive/My Drive/app/scripts/magnitude.py\" .\n",
        "  !cp \"drive/My Drive/app/data/magnitude/glove-lemmatized.6B.100d.magnitude\" ."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MOcfRyp0unn",
        "outputId": "714f9369-8ecb-4b9e-a0d1-c22abc4e1983"
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade wheel\n",
        "!pip install --upgrade build\n",
        "!pip install pymagnitude"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-21.2.4-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-21.2.4\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting build\n",
            "  Downloading build-0.6.0.post1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: importlib-metadata>=0.22 in /usr/local/lib/python3.7/dist-packages (from build) (4.6.4)\n",
            "Requirement already satisfied: pep517>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from build) (0.11.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from build) (1.2.1)\n",
            "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.7/dist-packages (from build) (21.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.22->build) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.22->build) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=19.0->build) (2.4.7)\n",
            "Installing collected packages: build\n",
            "Successfully installed build-0.6.0.post1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting pymagnitude\n",
            "  Downloading pymagnitude-0.1.143.tar.gz (5.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4 MB 3.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pymagnitude\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmv2Iq-k1ANs"
      },
      "source": [
        "from tensorflow.keras.layers import Input, TimeDistributed, LSTM, Bidirectional, Concatenate, Lambda, Reshape, Add, Activation, Multiply, Dense\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from highway_layer import Highway\n",
        "from contexual_embedding import C2VecLayer\n",
        "import os\n",
        "from pymagnitude import Magnitude, MagnitudeUtils\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p1QlGvqEEzW"
      },
      "source": [
        "from keras.layers import Layer\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.layers.advanced_activations import Softmax\n",
        "\n",
        "class SimilarityMatrix(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(SimilarityMatrix, self).__init__(**kwargs)\n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    self.context_shape = input_shape[0]\n",
        "    self.question_shape = input_shape[1]\n",
        "\n",
        "    self.kernel = self.add_weight(name=\"kernel\",\n",
        "                                  shape=(3 * input_shape[0][2], 1),\n",
        "                                  initializer='uniform',\n",
        "                                  trainable=True)\n",
        "\n",
        "    super(SimilarityMatrix, self).build(input_shape)\n",
        "\n",
        "  def compute_similarity(self, repeated_context_vectors, repeated_query_vectors):\n",
        "\n",
        "    element_wise_multiply = repeated_context_vectors * repeated_query_vectors\n",
        "    concatenated_tensor = tf.concat(\n",
        "    [repeated_context_vectors, repeated_query_vectors, element_wise_multiply], axis=-1)\n",
        "    dot_product = K.squeeze(K.dot(concatenated_tensor, self.kernel), axis=-1)\n",
        "\n",
        "    return dot_product\n",
        "\n",
        "  def build_similarity_matrix(self, context, question):\n",
        "\n",
        "    num_context_words = K.shape(context)[1]\n",
        "    num_query_words = K.shape(question)[1]\n",
        "\n",
        "    context_dim_repeat = K.concatenate([[1, 1], [num_query_words], [1]], 0)\n",
        "    query_dim_repeat = K.concatenate([[1], [num_context_words], [1, 1]], 0)\n",
        "    repeated_context_vectors = K.tile(K.expand_dims(context, axis=2), context_dim_repeat)\n",
        "    repeated_query_vectors = K.tile(K.expand_dims(question, axis=1), query_dim_repeat)\n",
        "    similarity_matrix = self.compute_similarity(repeated_context_vectors, repeated_query_vectors)\n",
        "    # similarity_matrix = tf.reshape(similarity_matrix, [self.context_shape[0],self.context_shape[1],self.question_shape[1]])\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "  def call(self, x):\n",
        "    context, question = x\n",
        "    self.similarity_matrix = self.build_similarity_matrix(context, question)\n",
        "    return self.similarity_matrix\n",
        "  \n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0][0],input_shape[0][1],input_shape[1][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP3-D3_YERn-"
      },
      "source": [
        "from keras.layers import Layer\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.layers.advanced_activations import Softmax\n",
        "\n",
        "class C2Q_Layer(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(C2Q_Layer, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    super(C2Q_Layer, self).build(input_shape)\n",
        "\n",
        "  def call(self,x):\n",
        "    similarity_matrix, question=x\n",
        "    attention = tf.nn.softmax(similarity_matrix)\n",
        "\n",
        "    self.U_A=K.sum(K.dot(attention,question),-2)\n",
        "\n",
        "    return self.U_A\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return self.U_A.shape;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6fMd07sETMi"
      },
      "source": [
        "from keras.layers import Layer\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.layers.advanced_activations import Softmax\n",
        "\n",
        "class Q2C_Layer(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(Q2C_Layer, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    super(Q2C_Layer, self).build(input_shape)\n",
        "\n",
        "  def call(self,x):\n",
        "    similarity_matrix, context=x\n",
        "    attention = tf.nn.softmax(K.max(similarity_matrix,axis=-1))\n",
        "\n",
        "    temp=K.expand_dims(K.sum(K.dot(attention,context),-2),1)\n",
        "\n",
        "    H_A=K.tile(temp,[1,similarity_matrix.shape[1],1])\n",
        "\n",
        "    return H_A\n",
        "  \n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return self.H_A.shape;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egh_3e8LEWuS"
      },
      "source": [
        "from keras.layers import Layer\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.layers.advanced_activations import Softmax\n",
        "\n",
        "class MegaMerge(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MegaMerge, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    super(MegaMerge, self).build(input_shape)\n",
        "\n",
        "  def call(self,x):\n",
        "    context,c2q,q2c=x\n",
        "    self.G=K.concatenate([context,c2q,context*c2q,context*q2c],axis=-1)\n",
        "\n",
        "    return self.G;\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return self.G.shape;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYrYcVlQEsiz"
      },
      "source": [
        "from keras.layers import Layer,LSTM,Bidirectional\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class ModellingLayer(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(ModellingLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.shape=input_shape\n",
        "  \n",
        "        self.lstm1 = Bidirectional(LSTM(int(input_shape[2]//8),\n",
        "                                   activation='tanh',\n",
        "                                   input_shape=(input_shape[1],input_shape[2]),\n",
        "                                   return_sequences=True, trainable=True))\n",
        "        self.lstm2 = Bidirectional(LSTM(int(input_shape[2]//8),\n",
        "                                   activation='tanh',\n",
        "                                   input_shape=(input_shape[1], int(input_shape[2]//4)),\n",
        "                                   return_sequences=True, trainable=True))\n",
        "        super(ModellingLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        \n",
        "        self.M1=self.lstm1(x)\n",
        "        \n",
        "        self.M2=self.lstm2(self.M1)\n",
        "        \n",
        "        self.temp1=tf.concat([x, self.M1], -1)\n",
        "        self.temp2=tf.concat([x, self.M2], -1)\n",
        "        \n",
        "        return self.temp1,self.temp2\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return self.temp1.shape,self.temp2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tCb_vpQEtow"
      },
      "source": [
        "from keras.layers import Layer\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "class OutputLayer(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(OutputLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "  \n",
        "        self.w1=self.add_weight(name=\"w1\",\n",
        "                                shape=(input_shape[0][2],),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "        self.w2=self.add_weight(name=\"w2\",\n",
        "                                shape=(input_shape[0][2],),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "\n",
        "        super(OutputLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        answer_span1=tf.tensordot(x[0],tf.transpose(self.w1),1)\n",
        "        answer_span2=tf.tensordot(x[1], tf.transpose(self.w2), 1)\n",
        "        \n",
        "        self.p1=tf.nn.softmax(answer_span1)\n",
        "        self.p2=tf.nn.softmax(answer_span2)\n",
        "        \n",
        "        \n",
        "        return self.p1,self.p2\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return self.p1.shape,self.p2.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWrKQs-h1-5-"
      },
      "source": [
        "class MagnitudeVectors:\n",
        "\n",
        "    def __init__(self):\n",
        "        # print(\"heloo\")\n",
        "        # base_dir = \"F://Pycharm Projects//bidaf-keras-master//data//magnitude\"\n",
        "        glove = Magnitude(\"glove-lemmatized.6B.100d.magnitude\")\n",
        "        fasttext = Magnitude(\"wiki-news-300d-1M.magnitude\")\n",
        "        self.vectors = Magnitude(glove,fasttext)\n",
        "\n",
        "    def load_vectors(self):\n",
        "        return self.vectors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVUTxmAA2ChA"
      },
      "source": [
        "from keras.callbacks import *\n",
        "class BidirectionalAttentionFlow():\n",
        "\n",
        "    def __init__(self, em_dim, max_passage_length=None, max_query_length=None):\n",
        "        self.em_dim = em_dim\n",
        "        self.max_passage_length = max_passage_length\n",
        "        self.max_query_length = max_query_length\n",
        "        with tf.device('/device:GPU:0'):\n",
        "            passage_input = Input(shape=(self.max_passage_length, em_dim), dtype='float32', name=\"passage_input\")\n",
        "            question_input = Input(shape=(self.max_query_length, em_dim), dtype='float32', name=\"question_input\")\n",
        "            question_embedding = question_input\n",
        "            passage_embedding = passage_input\n",
        "\n",
        "            highway_layers = Highway(name='highway_layer')\n",
        "            question_embedding = highway_layers(question_embedding)\n",
        "            passage_embedding = highway_layers(passage_embedding)\n",
        "\n",
        "            encoder_layer1 = Bidirectional(LSTM(em_dim, activation=\"tanh\", recurrent_activation=\"sigmoid\",\n",
        "                                                return_sequences=True), name='bidirectional_encoder')\n",
        "            encoded_question = encoder_layer1(question_embedding)\n",
        "            # encoder_layer2 = Bidirectional(LSTM(em_dim,\n",
        "            #                                   return_sequences=True), name='bidirectional_encoder1')\n",
        "            encoded_passage = encoder_layer1(passage_embedding)\n",
        "\n",
        "            sim = SimilarityMatrix(name=\"sm\")([encoded_passage, encoded_question])\n",
        "            c2q = C2Q_Layer(name=\"c2q\")([sim, encoded_question])\n",
        "            q2c = Q2C_Layer(name=\"q2c\")([sim, encoded_passage])\n",
        "            megamerge = MegaMerge(name=\"mega\")([encoded_passage, c2q, q2c])\n",
        "            t1, t2 = ModellingLayer(name=\"modelling\")(megamerge)\n",
        "            p1, p2 = OutputLayer(name=\"output\")([t1, t2])\n",
        "\n",
        "            model = Model(inputs=[passage_input, question_input], outputs=[p1, p2])\n",
        "            model.summary()\n",
        "            model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "            for i, w in enumerate(model.weights): print(i, w.name)\n",
        "            self.model = model\n",
        "\n",
        "    def load_bidaf(self, path):\n",
        "        custom_objects = {\n",
        "            \"Highway\": Highway,\n",
        "            \"SimilarityMatrix\": SimilarityMatrix,\n",
        "            \"C2Q_Layer\": C2Q_Layer,\n",
        "            \"Q2C_Layer\": Q2C_Layer,\n",
        "            \"MegaMerge\": MegaMerge,\n",
        "            \"ModellingLayer\": ModellingLayer,\n",
        "            \"OutputLayer\": OutputLayer\n",
        "        }\n",
        "\n",
        "        self.model = load_model(path, custom_objects=custom_objects)\n",
        "\n",
        "    def predict(self, c, q):\n",
        "        p1, p2 = self.model.predict(x={\"passage_input\": c, \"question_input\": q}, batch_size=1)\n",
        "        return p1, p2\n",
        "\n",
        "    def train_model(self, x1, x2, y1, y2, epochs=1):\n",
        "        with tf.device('/device:GPU:0'):\n",
        "            filepath=\"drive/My Drive/app/bidaf_{epoch:03d}.h5\"\n",
        "            checkpoint = ModelCheckpoint(filepath, verbose=1)\n",
        "            callbacks_list = [checkpoint]\n",
        "            history = self.model.fit(x={\"passage_input\": x1, \"question_input\": x2}, y={\"output\": y1, \"output_1\": y2},\n",
        "                                     batch_size=10,\n",
        "                                     epochs=epochs, verbose=2,\n",
        "                                     callbacks=callbacks_list)\n",
        "            self.model.save('drive/My Drive/app/bidaf250_21.h5')\n",
        "            return history, self.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpCNaCQPMg8Q"
      },
      "source": [
        "    with tf.device('/device:GPU:0'):\n",
        "        context_data = []\n",
        "        question_data = []\n",
        "        output_data1 = []\n",
        "        output_data2 = []\n",
        "        i = 0\n",
        "        sample_size = 5000\n",
        "        vectors = MagnitudeVectors().load_vectors()\n",
        "        with open(\"train_6.context\", 'r', encoding='utf8') as context_file, \\\n",
        "                open(\"train_6.question\", 'r', encoding='utf8') as question_file, \\\n",
        "                open(\"train_6.span\", 'r', encoding='utf8') as span_file:\n",
        "\n",
        "            for context, question, span in zip(tqdm(context_file, total=sample_size),\n",
        "                                               tqdm(question_file, total=sample_size),\n",
        "                                               tqdm(span_file, total=sample_size)):\n",
        "\n",
        "                context = [context]\n",
        "                question = [question]\n",
        "                passage = [pas.strip() for pas in context]\n",
        "                cont = []\n",
        "                for pas in passage:\n",
        "                    context_tokens = pas.split(\" \")\n",
        "                    cont.append(context_tokens)\n",
        "                original_passage = [pas.lower() for pas in passage]\n",
        "                quest = []\n",
        "                for ques in question:\n",
        "                    question_tokens = ques.split(\" \")\n",
        "                    quest.append(question_tokens)\n",
        "                context_batch = vectors.query(cont)\n",
        "                question_batch = vectors.query(quest)\n",
        "                pad1 = np.zeros(shape=(1, 250 - len(cont[0]), 400))\n",
        "                context_batch = np.concatenate((context_batch, pad1), 1)\n",
        "\n",
        "                pad2 = np.zeros(shape=(1, 20 - len(quest[0]), 400))\n",
        "                question_batch = np.concatenate((question_batch, pad2), 1)\n",
        "                answer_span = span.split()\n",
        "                output1 = np.zeros(shape=(1, 250), dtype=float)\n",
        "                output2 = np.zeros(shape=(1, 250), dtype=float)\n",
        "                output1[0][int(answer_span[0])] = 1\n",
        "                output2[0][int(answer_span[1])] = 1\n",
        "                # context_batch = tf.convert_to_tensor(context_batch, tf.float32)\n",
        "                # question_batch = tf.convert_to_tensor(question_batch, tf.float32)\n",
        "                # output1 = tf.convert_to_tensor(output1, tf.float32)\n",
        "                # output2 = tf.convert_to_tensor(output2, tf.float32)\n",
        "\n",
        "                context_data.append(context_batch)\n",
        "                question_data.append(question_batch)\n",
        "                output_data1.append(output1)\n",
        "                output_data2.append(output2)\n",
        "                # bidef.train_model(context_batch, question_batch, output, 5)\n",
        "                i = i + 1\n",
        "                if i == sample_size:\n",
        "                    break\n",
        "                print(i)\n",
        "\n",
        "        context_data = np.array(context_data)\n",
        "        context_data = np.reshape(context_data, (sample_size, 250, 400))\n",
        "        question_data = np.array(question_data)\n",
        "        question_data = np.reshape(question_data, (sample_size, 20, 400))\n",
        "        output_data1 = np.array(output_data1)\n",
        "        output_data1 = np.reshape(output_data1, (sample_size, 250))\n",
        "        output_data2 = np.array(output_data2)\n",
        "        output_data2 = np.reshape(output_data2, (sample_size, 250))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHzfPNEv4CxZ"
      },
      "source": [
        "\n",
        "bidef = BidirectionalAttentionFlow(400, 250, 20)\n",
        "bidef.load_bidaf('drive/My Drive/app/bidaf250_20.h5')\n",
        "print(\"loaded\")\n",
        "bidef.train_model(context_data, question_data, output_data1, output_data2, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vellTYU_8WD"
      },
      "source": [
        "vectors = MagnitudeVectors().load_vectors()\n",
        "\n",
        "\n",
        "def processForModel(context, question, span):\n",
        "        context = [context]\n",
        "        question = [question]\n",
        "        passage = [pas.strip() for pas in context]\n",
        "        cont = []\n",
        "        for pas in passage:\n",
        "            context_tokens = pas.split(\" \")\n",
        "            cont.append(context_tokens)\n",
        "        original_passage = [pas.lower() for pas in passage]\n",
        "        quest = []\n",
        "        for ques in question:\n",
        "            question_tokens = ques.split(\" \")\n",
        "            quest.append(question_tokens)\n",
        "        context_batch = vectors.query(cont)\n",
        "        question_batch = vectors.query(quest)\n",
        "        pad1 = np.zeros(shape=(1, 250 - len(cont[0]), 400))\n",
        "        context_batch = np.concatenate((context_batch, pad1), 1)\n",
        "\n",
        "        pad2 = np.zeros(shape=(1, 20 - len(quest[0]), 400))\n",
        "        question_batch = np.concatenate((question_batch, pad2), 1)\n",
        "        answer_span = span.split()\n",
        "        output1 = np.zeros(shape=(1, 250), dtype=float)\n",
        "        output2 = np.zeros(shape=(1, 250), dtype=float)\n",
        "        output1[0][int(answer_span[0])] = 1\n",
        "        output2[0][int(answer_span[1])] = 1\n",
        "        # context_batch = tf.convert_to_tensor(context_batch, tf.float32)\n",
        "        # question_batch = tf.convert_to_tensor(question_batch, tf.float32)\n",
        "        # output1 = tf.convert_to_tensor(output1, tf.float32)\n",
        "        # output2 = tf.convert_to_tensor(output2, tf.float32)\n",
        "\n",
        "        return context_batch, question_batch, output1, output2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk8AcrwP_Nqn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa165277-4e1a-45e3-ca01-d573f50042f3"
      },
      "source": [
        "bidef = BidirectionalAttentionFlow(400, 250, 20)\n",
        "bidef.load_bidaf('drive/My Drive/app/bidaf250_15.h5')\n",
        "context = \"in four months , aonuma 's team managed to present realistic horseback riding , [ l ] which nintendo later revealed to the public with a trailer at electronic entertainment expo 2004 . the game was scheduled to be released the next year , and was no longer a follow-up to the wind waker ; a true sequel to it was released for the nintendo ds in 2007 , in the form of phantom hourglass . miyamoto explained in interviews that the graphical style was chosen to satisfy demand , and that it better fit the theme of an older incarnation of link . the game runs on a modified the wind waker engine .\"\n",
        "question = \"where did nintendo preview the horseback riding feature ?\"\n",
        "context = 'from the beginning of 2014 , madonna began to make multiple media appearances . she appeared at the 56th annual grammy awards in january 2014 , performing \" open your heart \" alongside rappers macklemore & ryan lewis and singer mary lambert , who sang their single \" same love \" , as 33 couples were wed onstage , officiated by queen latifah . days later , she joined singer miley cyrus on her mtv unplugged special , singing a mash-up of \" do n\\'t tell me \" and cyrus \\' single \" we ca n\\'t stop \" ( 2013 ) . she also extended her business ventures and in february 2014 the singer premiered mdna skin , a range of skin care products , in tokyo , japan . after visiting her hometown of detroit during may 2014 , madonna decided to contribute funds to three of the city \\'s organizations , to help eliminate poverty from there . the singer released a statement saying that she was inspired by their work , adding that \" it was obvious to me that i had to get involved and be part of the solution to help detroit recover \" .'\n",
        "question=\"which singer did she join on mtv unplugged to sing ?\"\n",
        "span = \"70 71\"\n",
        "c, q, s1, s2 = processForModel(context, question, span)\n",
        "p1, p2 = bidef.predict(c, q)\n",
        "\n",
        "print(np.amax(p1, axis=1))\n",
        "print(np.amax(p2, axis=1))\n",
        "\n",
        "print(p1, p2)\n",
        "print(s1, s2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "passage_input (InputLayer)      [(None, 250, 400)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "question_input (InputLayer)     [(None, 20, 400)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "highway_layer (Highway)         multiple             320800      question_input[0][0]             \n",
            "                                                                 passage_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_encoder (Bidirect multiple             2563200     highway_layer[0][0]              \n",
            "                                                                 highway_layer[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sm (SimilarityMatrix)           (None, 250, 20)      2400        bidirectional_encoder[1][0]      \n",
            "                                                                 bidirectional_encoder[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "c2q (C2Q_Layer)                 (None, 250, 800)     0           sm[0][0]                         \n",
            "                                                                 bidirectional_encoder[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "q2c (Q2C_Layer)                 (None, 250, 800)     0           sm[0][0]                         \n",
            "                                                                 bidirectional_encoder[1][0]      \n",
            "__________________________________________________________________________________________________\n",
            "mega (MegaMerge)                (None, 250, 3200)    0           bidirectional_encoder[1][0]      \n",
            "                                                                 c2q[0][0]                        \n",
            "                                                                 q2c[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "modelling (ModellingLayer)      ((None, 250, 4000),  15366400    mega[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "output (OutputLayer)            ((None, 250), (None, 8000        modelling[0][0]                  \n",
            "                                                                 modelling[0][1]                  \n",
            "==================================================================================================\n",
            "Total params: 18,260,800\n",
            "Trainable params: 18,260,800\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "0 highway_layer/dense/kernel:0\n",
            "1 highway_layer/dense/bias:0\n",
            "2 highway_layer/dense_1/kernel:0\n",
            "3 highway_layer/dense_1/bias:0\n",
            "4 bidirectional_encoder/forward_lstm_7/lstm_cell_43/kernel:0\n",
            "5 bidirectional_encoder/forward_lstm_7/lstm_cell_43/recurrent_kernel:0\n",
            "6 bidirectional_encoder/forward_lstm_7/lstm_cell_43/bias:0\n",
            "7 bidirectional_encoder/backward_lstm_7/lstm_cell_44/kernel:0\n",
            "8 bidirectional_encoder/backward_lstm_7/lstm_cell_44/recurrent_kernel:0\n",
            "9 bidirectional_encoder/backward_lstm_7/lstm_cell_44/bias:0\n",
            "10 sm/kernel:0\n",
            "11 modelling/bidirectional/forward_lstm/lstm_cell_1/kernel:0\n",
            "12 modelling/bidirectional/forward_lstm/lstm_cell_1/recurrent_kernel:0\n",
            "13 modelling/bidirectional/forward_lstm/lstm_cell_1/bias:0\n",
            "14 modelling/bidirectional/backward_lstm/lstm_cell_2/kernel:0\n",
            "15 modelling/bidirectional/backward_lstm/lstm_cell_2/recurrent_kernel:0\n",
            "16 modelling/bidirectional/backward_lstm/lstm_cell_2/bias:0\n",
            "17 modelling/bidirectional_1/forward_lstm_1/lstm_cell_4/kernel:0\n",
            "18 modelling/bidirectional_1/forward_lstm_1/lstm_cell_4/recurrent_kernel:0\n",
            "19 modelling/bidirectional_1/forward_lstm_1/lstm_cell_4/bias:0\n",
            "20 modelling/bidirectional_1/backward_lstm_1/lstm_cell_5/kernel:0\n",
            "21 modelling/bidirectional_1/backward_lstm_1/lstm_cell_5/recurrent_kernel:0\n",
            "22 modelling/bidirectional_1/backward_lstm_1/lstm_cell_5/bias:0\n",
            "23 output/w1:0\n",
            "24 output/w2:0\n",
            "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9470757c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[0.0631331]\n",
            "[0.0912025]\n",
            "[[5.24969594e-03 4.41770993e-03 4.04510322e-03 7.72981598e-03\n",
            "  4.87375206e-03 6.81793237e-03 1.66639354e-03 2.54868051e-03\n",
            "  8.03650793e-03 1.92285737e-02 1.24455262e-02 8.20558305e-03\n",
            "  1.02402942e-02 1.81105341e-02 7.97433219e-03 1.40524336e-02\n",
            "  2.75463411e-02 2.39644732e-02 5.10343906e-03 2.79793249e-03\n",
            "  1.38376427e-03 1.21503670e-03 1.82651643e-03 6.71694351e-04\n",
            "  6.43924873e-04 1.22358526e-03 2.61774458e-03 3.86665951e-03\n",
            "  9.48792324e-04 3.74756345e-04 3.50610460e-04 1.76881615e-03\n",
            "  4.70556033e-03 9.44482549e-03 7.42825349e-03 9.42988953e-03\n",
            "  5.55812501e-03 2.76205265e-03 1.01124777e-02 1.58837510e-02\n",
            "  1.95344318e-03 3.26440995e-04 6.09568233e-04 1.31090385e-03\n",
            "  1.72885900e-03 2.82331406e-03 2.88873553e-03 9.78707072e-03\n",
            "  3.90692741e-03 2.72789712e-03 3.13856170e-03 3.71286163e-03\n",
            "  6.78810398e-03 8.97194492e-04 6.51712862e-04 1.30080811e-03\n",
            "  1.18301578e-03 1.11718427e-03 2.72214270e-03 5.20433295e-03\n",
            "  1.14822371e-02 4.54443647e-03 1.40128820e-03 2.47820373e-03\n",
            "  2.85631401e-03 3.46313743e-03 1.47001254e-02 1.52946618e-02\n",
            "  3.18827150e-02 6.31330970e-02 2.10098376e-02 7.53348437e-03\n",
            "  3.20156007e-02 3.14062116e-02 1.37182200e-02 8.05111210e-03\n",
            "  3.77587594e-03 7.55071041e-03 4.21150174e-03 1.91608899e-03\n",
            "  5.90380151e-04 8.56429778e-04 9.28053911e-04 7.34410023e-04\n",
            "  4.22713536e-04 4.37307566e-04 4.04646765e-04 7.95174135e-04\n",
            "  1.03240574e-03 3.37104565e-04 6.33244180e-04 1.01619419e-03\n",
            "  5.02744872e-03 4.57501324e-03 3.61157302e-03 2.83405957e-03\n",
            "  7.04003610e-03 1.56038642e-02 2.19797731e-02 1.28524504e-02\n",
            "  1.52550348e-02 2.15789711e-02 6.87604308e-03 6.38496775e-03\n",
            "  5.08745659e-03 2.73283553e-03 9.94716910e-04 1.09578399e-03\n",
            "  1.07642737e-03 1.82852598e-03 1.05198287e-03 1.85392110e-03\n",
            "  2.67331367e-03 5.03078469e-03 1.01298863e-02 2.40313749e-03\n",
            "  9.40730829e-04 2.37039472e-03 2.13638140e-03 1.61108260e-03\n",
            "  4.81840879e-03 2.42484434e-03 1.65305439e-03 5.01150177e-03\n",
            "  1.27345581e-02 2.46947925e-02 5.46363946e-03 8.69545452e-03\n",
            "  3.59126089e-03 6.35277930e-03 7.14449666e-03 4.98409625e-03\n",
            "  2.11083336e-03 2.09031599e-03 3.02864530e-03 5.85947207e-04\n",
            "  1.26157341e-03 1.59605183e-03 1.47120603e-03 2.67679946e-03\n",
            "  6.19373451e-04 8.80286679e-04 1.53752621e-03 1.25046556e-03\n",
            "  1.23030044e-03 2.20345466e-03 5.97658929e-04 1.16210286e-03\n",
            "  1.09557708e-03 1.04117272e-03 1.31492770e-03 1.77406595e-03\n",
            "  3.56444904e-03 3.97428067e-03 2.37404404e-03 1.75752405e-03\n",
            "  1.10325483e-03 1.56836135e-03 1.13688377e-03 1.95885386e-03\n",
            "  9.37517931e-04 8.16323443e-04 1.70712118e-03 8.78259832e-04\n",
            "  9.71334980e-04 1.23300197e-03 8.89224739e-04 2.72080443e-04\n",
            "  3.41380292e-04 4.89539021e-04 9.37354335e-04 5.95288484e-04\n",
            "  7.49804748e-04 2.02952672e-03 1.67983037e-03 3.34298995e-03\n",
            "  5.58707745e-03 3.52851744e-03 5.93131966e-03 3.53803799e-03\n",
            "  3.92619622e-03 1.03565471e-03 5.38923645e-04 7.27407564e-05\n",
            "  8.88819215e-05 1.25930862e-04 9.41199839e-05 1.16521142e-04\n",
            "  1.27187927e-04 1.74882383e-04 2.72448381e-04 1.06845149e-03\n",
            "  1.23582775e-03 2.31975176e-03 5.64545382e-03 5.00465182e-03\n",
            "  1.14446196e-03 1.86223632e-03 3.98852847e-03 4.45593430e-03\n",
            "  1.13872140e-03 5.46264575e-04 4.01094898e-04 3.53067362e-04\n",
            "  3.28029607e-04 3.03186860e-04 2.73606270e-04 2.40538914e-04\n",
            "  2.06198770e-04 1.72393641e-04 1.40632738e-04 1.12207263e-04\n",
            "  8.80056034e-05 6.83502331e-05 5.30353193e-05 4.15147336e-05\n",
            "  3.31132347e-05 2.71811149e-05 2.31775782e-05 2.07015096e-05\n",
            "  1.94937163e-05 1.94279188e-05 2.04978854e-05 2.27970725e-05\n",
            "  2.64741498e-05 3.16356387e-05 3.81704760e-05 4.55196491e-05\n",
            "  5.25193939e-05 5.75346453e-05 5.90049232e-05 5.61913584e-05\n",
            "  4.96196333e-05 4.08521051e-05 3.17454887e-05 2.37259597e-05\n",
            "  1.74849972e-05 1.30979197e-05 1.03230163e-05 8.87224560e-06\n",
            "  8.58968747e-06 9.57492046e-06 1.22944701e-05 1.76169508e-05\n",
            "  2.66611739e-05 4.09760427e-05 6.55818170e-05 1.20372281e-04\n",
            "  2.65971589e-04 6.26720719e-04]] [[6.23014364e-04 5.59645630e-04 3.69298612e-04 2.09737900e-03\n",
            "  7.31975719e-04 2.76246688e-03 1.56055007e-03 1.01703125e-03\n",
            "  1.51012381e-03 2.08690366e-03 3.00644065e-03 6.82901539e-03\n",
            "  4.88810729e-03 6.57820179e-03 4.38003122e-03 5.92040788e-03\n",
            "  9.21512725e-03 1.94958742e-02 1.06597767e-02 9.89155040e-03\n",
            "  1.07188995e-02 2.43018134e-03 3.98528950e-03 2.97336246e-03\n",
            "  1.32463489e-03 1.67330880e-03 1.28137295e-03 8.87815732e-04\n",
            "  7.59805901e-04 1.10464415e-03 6.24400522e-04 5.03140285e-04\n",
            "  8.83584448e-04 1.16167844e-03 1.42010037e-03 3.77634063e-03\n",
            "  1.76603293e-02 1.51620350e-02 3.01065970e-02 1.93481836e-02\n",
            "  4.62133209e-02 1.90815596e-02 5.75619210e-03 1.88351140e-03\n",
            "  1.23095319e-03 3.13477272e-03 1.63399613e-03 3.09597859e-03\n",
            "  3.78651180e-03 5.85119726e-03 3.63712708e-03 2.32141064e-03\n",
            "  6.48593871e-03 6.51264848e-03 2.68118582e-03 7.07868514e-03\n",
            "  4.65713934e-03 2.12342864e-03 2.69449341e-03 1.11742412e-03\n",
            "  2.66976143e-03 5.23937529e-03 2.96194870e-03 2.40587866e-03\n",
            "  1.67660662e-03 9.07698837e-04 3.63857210e-03 4.47833174e-03\n",
            "  1.95626944e-02 2.71972296e-02 9.12025039e-02 3.04758377e-02\n",
            "  2.64685777e-02 4.11385912e-02 3.06576452e-02 4.40460675e-02\n",
            "  1.62019702e-02 1.27644476e-02 5.18068070e-03 6.25319897e-03\n",
            "  2.18942669e-03 1.66943379e-03 5.81578743e-04 4.91561286e-04\n",
            "  2.87226236e-04 5.28746837e-04 5.63133325e-04 3.69153691e-04\n",
            "  2.05000890e-03 1.69964606e-03 2.79714646e-03 9.37689461e-04\n",
            "  9.89263118e-04 1.85387243e-03 2.98751259e-03 4.65502360e-03\n",
            "  4.80965100e-03 2.86513662e-03 8.03484485e-03 4.72689208e-03\n",
            "  6.35899757e-03 6.86733297e-03 5.97520434e-03 6.69876985e-03\n",
            "  9.98059170e-03 1.79878772e-02 1.34588936e-02 3.79581798e-03\n",
            "  2.20200541e-03 2.04827901e-03 1.18005313e-03 5.86086181e-04\n",
            "  1.32733151e-03 1.12941512e-03 3.67149001e-03 5.66495474e-03\n",
            "  1.73415612e-03 1.39984365e-03 1.61335759e-03 6.08088581e-04\n",
            "  1.82880380e-03 2.66426250e-03 3.35053835e-03 2.49293622e-03\n",
            "  2.50262768e-03 9.66414204e-03 4.97942071e-03 1.53209743e-02\n",
            "  8.29944472e-03 5.03754001e-03 2.64309956e-03 4.48620713e-03\n",
            "  3.82313168e-03 1.58996360e-03 5.73148884e-03 2.88710704e-03\n",
            "  1.42678353e-03 3.92242101e-03 1.15481405e-03 3.26729987e-03\n",
            "  1.70874458e-03 6.79355765e-04 1.12864630e-03 1.24039240e-03\n",
            "  3.20784735e-04 7.98960640e-04 2.41216774e-04 3.90669967e-04\n",
            "  5.76679600e-04 4.70418394e-04 1.40554317e-03 9.12376261e-04\n",
            "  7.98870827e-04 1.30555975e-03 1.36171521e-03 1.66792261e-03\n",
            "  7.22060151e-04 1.96789983e-03 1.12988658e-03 6.60805192e-04\n",
            "  9.72418987e-04 5.02778967e-04 5.92062917e-04 1.32014855e-03\n",
            "  9.49920837e-04 6.14201838e-04 5.56952670e-04 2.48840723e-04\n",
            "  4.51433589e-04 1.75178046e-04 1.62197085e-04 1.87842079e-04\n",
            "  1.02274919e-04 1.95637744e-04 2.57738460e-04 1.59985262e-04\n",
            "  2.74414933e-04 1.81455373e-04 1.10165307e-03 5.41003869e-04\n",
            "  1.94270434e-03 6.23434611e-04 8.27502427e-04 2.80338797e-04\n",
            "  2.76852800e-04 3.53981351e-04 2.68116599e-04 8.48338007e-05\n",
            "  1.00367700e-04 1.25572967e-04 6.06276185e-05 1.58735871e-04\n",
            "  3.46805540e-04 1.84646804e-04 4.92400584e-04 1.58285768e-03\n",
            "  3.10517419e-03 2.93961505e-03 2.74857840e-03 2.31837009e-03\n",
            "  1.42414636e-03 1.15798579e-03 9.54816847e-04 8.01402748e-04\n",
            "  6.73011980e-04 5.51149185e-04 4.33913640e-04 3.29038077e-04\n",
            "  2.43353854e-04 1.78167845e-04 1.30603556e-04 9.64317284e-05\n",
            "  7.18251345e-05 5.39075435e-05 4.06927587e-05 3.08505176e-05\n",
            "  2.34856558e-05 1.79788778e-05 1.38853555e-05 1.08745667e-05\n",
            "  8.69585142e-06 7.15865892e-06 6.12068608e-06 5.48003894e-06\n",
            "  5.16919305e-06 5.14912802e-06 5.40173644e-06 5.91777777e-06\n",
            "  6.67736734e-06 7.62258024e-06 8.62954190e-06 9.49863873e-06\n",
            "  9.98614706e-06 9.88432328e-06 9.11897733e-06 7.80253891e-06\n",
            "  6.19681783e-06 4.60278479e-06 3.24777609e-06 2.23288159e-06\n",
            "  1.55030780e-06 1.13673379e-06 9.24806081e-07 8.76855374e-07\n",
            "  1.00777266e-06 1.44351084e-06 2.72538374e-06 7.50639846e-06\n",
            "  3.20256096e-05 1.98300795e-04]]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}